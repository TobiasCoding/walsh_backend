

=== File: Dockerfile ===
FROM python:3.10-slim

# Instalar dependencias del sistema necesarias para confluent-kafka
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    librdkafka-dev \
    python3-dev \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copiar e instalar dependencias Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar el código fuente
COPY . .

# Exponer el puerto 8005 (o el que elijas)
EXPOSE 8005

# Comando por defecto
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8005"]


=== File: app/__init__.py ===


=== File: app/kafka_producer.py ===
import json
from confluent_kafka import Producer
from datetime import datetime, timezone
import os

# VARIABLES DE KAFKA
# Se leen de entorno si están definidas, sino usamos valores por defecto:
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092")
KAFKA_TOPIC = os.getenv("KAFKA_TOPIC", "configUpdate")

producer: Producer | None = None


def init_kafka_producer():
    """
    Inicializa un Producer global conectado al broker de Kafka.
    """
    global producer
    if producer is None:
        producer = Producer({"bootstrap.servers": KAFKA_BOOTSTRAP_SERVERS})


# En app/kafka_producer.py, reemplaza test_kafka_connection() por:
def test_kafka_connection():
    if producer is None:
        raise RuntimeError("Producer no inicializado")
    try:
        metadata = producer.list_topics(timeout=5.0)
    except Exception as e:
        print(f"[WARNING] No se pudo obtener metadata de Kafka: {e}")
        return
    # Si el tópico no existe, no intentamos crearlo aquí
    if KAFKA_TOPIC not in metadata.topics:
        print(
            f"[WARNING] Tópico '{KAFKA_TOPIC}' no encontrado, y no se crea automáticamente."
        )


def publish_config_update(service: str, version: int, payload: dict):
    """
    Publica un evento de configuración para 'service' con una versión y payload.
    :param service: nombre del microservicio (ej: "file_service")
    :param version: número incremental de versión
    :param payload: diccionario { "VAR1": "valor1", ... }
    """
    if producer is None:
        raise RuntimeError("Producer de Kafka no inicializado")

    message = {
        "service": service,
        "version": version,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "payload": payload,
    }

    producer.produce(
        topic=KAFKA_TOPIC,
        key=service.encode("utf-8"),
        value=json.dumps(message).encode("utf-8"),
    )
    # Podemos hacer poll para procesar callbacks, o flush si queremos sincrónico
    producer.poll(0)


=== File: app/main.py ===
from fastapi import FastAPI, HTTPException
from app.routers.config import router as config_router
from app.kafka_producer import (
    init_kafka_producer,
    test_kafka_connection,
    KAFKA_BOOTSTRAP_SERVERS,
    KAFKA_TOPIC,
)

app = FastAPI(title="admin_panel")


@app.on_event("startup")
def startup_event():
    """
    Al iniciar, levantamos el Producer y verificamos que el topic exista.
    Si Kafka no está disponible o el topic no se creó aún, solo se imprime el error.
    """
    try:
        init_kafka_producer()
        test_kafka_connection()
    except Exception as e:
        print(
            f"[WARNING] No se pudo conectar a Kafka o crear topic '{KAFKA_TOPIC}': {e}"
        )


app.include_router(config_router, prefix="/config", tags=["config"])


@app.get("/healthz")
async def healthz():
    return {
        "status": "ok",
        "kafka_bootstrap": KAFKA_BOOTSTRAP_SERVERS,
        "kafka_topic": KAFKA_TOPIC,
    }


=== File: app/models.py ===
from pydantic import BaseModel
from typing import Dict
from datetime import datetime


class ConfigUpdate(BaseModel):
    service: str
    version: int
    payload: Dict[str, str]
    timestamp: datetime

    class Config:
        json_schema_extra = {
            "example": {
                "service": "file_service",
                "version": 1,
                "timestamp": "2025-05-30T23:45:00Z",
                "payload": {
                    "FILE_BASE_PATH": "/data/files",
                    "KAFKA_BOOTSTRAP": "kafka:9092",
                    "KAFKA_TOPIC": "file_stored",
                    "MAX_UPLOAD_MB": "100",
                },
            }
        }


=== File: app/routers/config.py ===
from fastapi import APIRouter, HTTPException, status
from app.schemas import ConfigPayload
from app.kafka_producer import publish_config_update

router = APIRouter()


@router.post("/{service_name}", status_code=status.HTTP_200_OK)
async def update_config(service_name: str, body: ConfigPayload):
    """
    Endpoint: POST /config/{service_name}
    Recibe JSON {"version": int, "payload": { key: value, ... }}.
    Publica un mensaje en Kafka para que el microservicio 'service_name'
    reciba en tiempo real la configuración.
    """
    try:
        publish_config_update(
            service=service_name, version=body.version, payload=body.payload
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error al publicar actualización de configuración: {e}",
        )

    return {"detail": f"Configuración enviada para servicio '{service_name}'"}


=== File: app/schemas.py ===
from pydantic import BaseModel
from typing import Dict


class ConfigPayload(BaseModel):
    version: int
    payload: Dict[str, str]

    class Config:
        json_schema_extra = {
            "example": {
                "version": 2,
                "payload": {
                    "FILE_BASE_PATH": "/data/files",
                    "KAFKA_BOOTSTRAP": "kafka:9092",
                    "KAFKA_TOPIC": "file_stored",
                    "MAX_UPLOAD_MB": "100",
                },
            }
        }


=== File: dev_docs/README.md ===
# admin_panel

Microservicio para gestionar configuración centralizada vía Kafka.

## Requisitos previos

- Python 3.10+
- Docker (solo para Kafka y Zookeeper)

## 1. Levantar Kafka y Zookeeper en Docker

```bash
# Desde la raíz del proyecto (o donde esté tu docker-compose.yml)
docker network create walsh_net         # si no existe
docker compose -f services/kafka/docker-compose.yml up -d
```


=== File: requirements.txt ===
fastapi==0.110.0
uvicorn[standard]==0.29.0
pydantic==2.6.4
confluent-kafka==2.3.0
python-dotenv==1.0.1
