

=== File: Dockerfile ===
FROM python:3.13-alpine

# Instalar dependencias necesarias para confluent-kafka y compilación de paquetes
RUN apk add --no-cache \
    gcc \
    musl-dev \
    libffi-dev \
    openssl-dev \
    librdkafka-dev

# Crear directorio de trabajo
WORKDIR /app

# Copiar e instalar dependencias Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar el resto del código
COPY . .

# Exponer el puerto usado por FastAPI
EXPOSE 8004

# Ejecutar el servidor
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8004"]


=== File: app/events.py ===
# app/events.py

from confluent_kafka import Producer
from datetime import datetime, timezone
from app.models import FileMeta, FileStoredEvent
from config.variables.file_storage import KAFKA_BOOTSTRAP, KAFKA_TOPIC
import json
import logging

logger = logging.getLogger(__name__)


def get_producer() -> Producer:
    return Producer({"bootstrap.servers": KAFKA_BOOTSTRAP})


def publish_file_stored_event(key: str, meta: FileMeta):
    producer = get_producer()
    event = FileStoredEvent(key=key, meta=meta, timestamp=datetime.now(timezone.utc))
    json_payload = event.model_dump_json()
    try:
        producer.produce(
            topic=KAFKA_TOPIC, key=key.encode(), value=json_payload.encode()
        )
        producer.poll(0)
    except Exception as e:
        logger.exception(f"Error al publicar evento Kafka: {e}")


def test_kafka_connection():
    producer = get_producer()
    metadata = producer.list_topics(timeout=5)
    if KAFKA_TOPIC not in metadata.topics:
        raise RuntimeError(f"Tópico {KAFKA_TOPIC} no existe")


=== File: app/main.py ===
# app/main.py

from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from prometheus_client import make_asgi_app
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.responses import JSONResponse
from app.routes.files import router as files_router
from app.events import test_kafka_connection
import os
from config.variables.security import ALLOWED_EXTENSIONS


settings = get_settings()
app = FastAPI(title="file_service")

# Middleware: CORS (si se necesita)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Middleware: limitador de tamaño de archivo
class UploadLimitMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        content_length = request.headers.get("content-length")
        if (
            content_length
            and int(content_length) > settings.MAX_UPLOAD_MB * 1024 * 1024
        ):
            return JSONResponse(
                status_code=413,
                content={
                    "detail": f"Archivo supera el límite de {settings.MAX_UPLOAD_MB} MB"
                },
            )
        return await call_next(request)


app.add_middleware(UploadLimitMiddleware)

# Rutas API
app.include_router(files_router, prefix="", tags=["files"])


# Rutas de salud
@app.get("/healthz")
async def healthz():
    # Verifica si se puede escribir en disco
    try:
        test_file = os.path.join(settings.FILE_BASE_PATH, ".healthcheck")
        with open(test_file, "w") as f:
            f.write("ok")
        os.remove(test_file)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Storage error: {e}")

    # Verifica conexión Kafka
    try:
        test_kafka_connection()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Kafka error: {e}")

    return {"status": "ok"}


# Rutas de métricas
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)


=== File: app/models.py ===
# app/models.py

from pydantic import BaseModel
from typing import List
from datetime import datetime


class FileMeta(BaseModel):
    user_id: str
    tags: List[str]
    sha256: str
    created_at: datetime


class FileStoredEvent(BaseModel):
    key: str
    meta: FileMeta
    timestamp: datetime


=== File: app/routes/files.py ===
# app/routes/files.py

from fastapi import APIRouter, UploadFile, File, HTTPException, Request
from fastapi.responses import StreamingResponse, Response
from hashlib import sha256
from datetime import datetime, timezone
from mimetypes import guess_type
import aiofiles, os
from app.models import FileMeta
from app.storage.local import LocalFSBackend
from app.events import publish_file_stored_event
from secrets import token_hex
from datetime import datetime
from secrets import token_hex
from config.variables.file_storage import FILE_BASE_PATH, FILE_PUBLIC_URL

router = APIRouter()
storage = LocalFSBackend(FILE_BASE_PATH)


def generar_clave_archivo(
    origen: str, extension: str, base_path: str, max_reintentos: int = 10
) -> str:
    fecha = datetime.now(timezone.utc).strftime("%Y%m%d")
    hora_min = datetime.now(timezone.utc).strftime("%H%M")
    for _ in range(max_reintentos):
        short_id = token_hex(3)  # 6 caracteres
        nombre_archivo = f"{hora_min}-{short_id}{extension}"
        clave = f"{origen}/{fecha}/{nombre_archivo}"
        ruta = os.path.join(base_path, clave)
        if not os.path.exists(ruta):
            return clave
    raise RuntimeError(
        f"No se pudo generar un nombre único después de {max_reintentos} intentos"
    )


@router.post("/files")
async def upload_file(request: Request, file: UploadFile = File(...)):
    user_id = request.headers.get("x-user-id", "anonymous")
    origen = request.headers.get("x-source", "unknown")  # microservicio llamante
    content = await file.read()

    extension = os.path.splitext(file.filename or "")[1].lower()

    try:
        key = generar_clave_archivo(origen, extension, FILE_BASE_PATH)
        storage.save_file(key, content)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error al guardar archivo: {e}")

    hash_hex = sha256(content).hexdigest()
    meta = FileMeta(
        user_id=user_id, tags=[], sha256=hash_hex, created_at=datetime.utcnow()
    )
    publish_file_stored_event(key, meta)

    url = f"{FILE_PUBLIC_URL}/{key}"
    return {"key": key, "url": url}


@router.get("/files/{key}")
async def get_file(key: str):
    try:
        path = os.path.join(FILE_BASE_PATH, key)
        if not os.path.exists(path):
            raise HTTPException(status_code=404, detail="Archivo no encontrado")

        mime, _ = guess_type(path)
        mime = mime or "application/octet-stream"

        async def file_iterator():
            async with aiofiles.open(path, mode="rb") as f:
                while chunk := await f.read(8192):
                    yield chunk

        return StreamingResponse(file_iterator(), media_type=mime)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error al leer archivo: {e}")


@router.head("/files/{key}")
async def head_file(key: str):
    path = os.path.join(FILE_BASE_PATH, key)
    if not os.path.exists(path):
        raise HTTPException(status_code=404, detail="Archivo no encontrado")

    size = os.path.getsize(path)
    mime, _ = guess_type(path)
    mime = mime or "application/octet-stream"

    headers = {
        "Content-Length": str(size),
        "Content-Type": mime,
    }
    return Response(headers=headers)


@router.delete("/files/{key}")
async def delete_file(key: str):
    path = os.path.join(FILE_BASE_PATH, key)
    if not os.path.exists(path):
        raise HTTPException(status_code=404, detail="Archivo no encontrado")
    try:
        os.remove(path)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"No se pudo eliminar: {e}")
    return {"detail": "Archivo eliminado"}


=== File: app/storage/backend.py ===
# app/storage/backend.py

from abc import ABC, abstractmethod


class StorageBackend(ABC):
    """Interfaz abstracta para backends de almacenamiento."""

    @abstractmethod
    def save_file(self, key: str, data: bytes) -> None:
        """Guarda un archivo binario bajo una clave única."""
        pass

    @abstractmethod
    def get_file(self, key: str) -> bytes:
        """Recupera un archivo binario por clave."""
        pass

    @abstractmethod
    def delete_file(self, key: str) -> None:
        """Elimina un archivo almacenado (lógico o físico)."""
        pass

    @abstractmethod
    def file_exists(self, key: str) -> bool:
        """Verifica si un archivo existe."""
        pass


=== File: app/storage/local.py ===
# app/storage/local.py

import os
from pathlib import Path
from app.storage.backend import StorageBackend


class LocalFSBackend(StorageBackend):
    """Implementación del backend que guarda archivos en el sistema de archivos local."""

    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)

    def _resolve_path(self, key: str) -> Path:
        return self.base_path / key

    def save_file(self, key: str, data: bytes) -> None:
        path = self._resolve_path(key)
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_bytes(data)

    def get_file(self, key: str) -> bytes:
        path = self._resolve_path(key)
        if not path.exists():
            raise FileNotFoundError(f"Archivo {key} no encontrado")
        return path.read_bytes()

    def delete_file(self, key: str) -> None:
        path = self._resolve_path(key)
        if path.exists():
            path.unlink()

    def file_exists(self, key: str) -> bool:
        return self._resolve_path(key).exists()


=== File: app/tests/test_files_api.py ===
# app/tests/test_files_api.py

import pytest
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)


def test_upload_and_download_file(tmp_path, monkeypatch):
    monkeypatch.setenv("FILE_BASE_PATH", str(tmp_path))

    content = b"contenido de prueba"
    response = client.post(
        "/files",
        files={"file": ("archivo.txt", content)},
        headers={"x-user-id": "test_user"},
    )
    assert response.status_code == 200
    data = response.json()
    assert "key" in data
    key = data["key"]

    # Obtener archivo
    get_response = client.get(f"/files/{key}")
    assert get_response.status_code == 200
    assert get_response.content == content


=== File: app/tests/test_storage.py ===


=== File: docker-compose.yml ===
version: '3.8'

services:
  file_service:
    build: .
    ports:
      - "8004:8004"
    environment:
      - FILE_BASE_PATH=/data/files
      - FILE_PUBLIC_URL=http://localhost:8004/files
      - KAFKA_BOOTSTRAP=kafka:9092
      - KAFKA_TOPIC=file_stored
      - MAX_UPLOAD_MB=50
    volumes:
      - ./data/files:/data/files
    depends_on:
      - kafka
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000


# DESCOMENTAR EN CASO DE USAR VOLUMENES DENTRO DEL CONTENEDOR
# volumes:
#   - file_data:/data/files

=== File: requirements.txt ===
fastapi==0.110.0
uvicorn[standard]==0.29.0
python-multipart==0.0.9
aiofiles==23.2.1
pydantic==2.6.4
pydantic-settings==2.2.1
confluent-kafka==2.3.0
prometheus-client==0.20.0
pytest==8.1.1
pytest-asyncio==0.23.6
httpx==0.27.0
starlette==0.27.0
python-dotenv==1.0.1
