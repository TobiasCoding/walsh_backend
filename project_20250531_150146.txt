

=== File: servers/admin_panel/config/env_loader.py ===
# config/env_loader.py
from dotenv import dotenv_values
from pathlib import Path

CONFIG_ENV_PATH = Path(__file__).parent / ".env"


def load_env_config() -> dict:
    """
    Carga variables desde config/.env como diccionario.
    """
    return dotenv_values(CONFIG_ENV_PATH)


=== File: README.md ===
Ver dev_docs


=== File: common/dev_docs/file_integrity.md ===
üõ°Ô∏è Medidas de seguridad que se pueden agregar
Escaneo antivirus (opcional, v√≠a ClamAV sidecar)

Detecta malware en archivos binarios.

Puede montarse como contenedor auxiliar o usarse v√≠a subprocess.

Verificaci√≥n MIME y contenido real

Validar que un .pdf sea realmente PDF y no un .exe renombrado.

Puede hacerse con magic o python-magic.

Lista blanca de extensiones permitidas

Rechazar tipos peligrosos (ej.: .exe, .sh, .bat, .js).

Firmado opcional o checksum desde el cliente

Comparar hash SHA-256 con uno provisto para validar integridad.

Protecci√≥n de rutas p√∫blicas (firma o token)

Si file_service expone archivos p√∫blicamente, se puede exigir una firma temporaria para acceder a GET /files/{key}.



=== File: common/logger.py ===
# common/logger.py
import logging, sys, json


class JSONFormatter(logging.Formatter):
    def format(self, record):
        log = {
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "service": "file_service",  # o set din√°mico desde env
            "message": record.getMessage(),
        }
        return json.dumps(log)


def get_logger(name="app", level=logging.INFO):
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(JSONFormatter())
    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.addHandler(handler)
    logger.propagate = False
    return logger


=== File: common/validation/file_integrity.py ===
# AJUSTAR ESTAS FUNCIONES CUANDO TENGA ALGUN INGESTOR YA PREPARADO PARA QUE INTERMEDIEN CON FILE_SERVICE


import hashlib
import subprocess
from pathlib import Path
import magic
import mimetypes
from config.variables.security import ALLOWED_EXTENSIONS


def sha256_matches(content: bytes, expected_hash: str) -> bool:
    return hashlib.sha256(content).hexdigest() == expected_hash


def is_allowed_extension(file_path: Path) -> bool:
    return file_path.suffix.lower() in ALLOWED_EXTENSIONS


def is_real_mime(file_path: Path) -> bool:
    try:
        mime = magic.from_file(str(file_path), mime=True)
        expected, _ = mimetypes.guess_type(file_path.name)
        return mime == expected
    except Exception:
        return False


def is_malicious(file_path: Path) -> bool:
    try:
        result = subprocess.run(
            ["clamscan", str(file_path)],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=10,
        )
        return "Infected files: 1" in result.stdout
    except Exception:
        return False


# FORMA DE LLAMAR AL MODULO

# from common.validation.file_integrity import is_malicious, is_allowed_extension

# if not is_allowed_extension(file_path) or is_malicious(file_path):
#     raise Exception("Archivo no permitido")


=== File: config/dev_docs/README.md ===



=== File: config/variables/file_storage.py ===
FILE_BASE_PATH: str = "data/files"
FILE_PUBLIC_URL: str = "http://localhost:8004/files"
KAFKA_BOOTSTRAP: str = "localhost:9092"
KAFKA_TOPIC: str = "file_stored"
MAX_UPLOAD_MB: int = 50
ENV_FILE = ".env"


=== File: config/variables/security.py ===
# config/security.py

# Lista blanca de extensiones permitidas
ALLOWED_EXTENSIONS = {
    ".pdf",
    ".txt",
    ".jpg",
    ".jpeg",
    ".png",
    ".docx",
    ".xlsx",
    ".csv",
    ".json",
    ".html",
    ".xml",
    ".zip",
}

MAX_UPLOAD_MB = 50

ENABLE_CLAMAV = True  # puede desactivarse en entornos de desarrollo


=== File: infra/cloudflare_rules.md ===
Document√°s las reglas que vas configurando en Cloudflare:

WAF (bloqueos de path, IPs, user-agents)

Rate limits

Redirecciones

Configuraci√≥n de cach√©

TLS/SSL mode (Flexible, Full, Full strict)

Protecci√≥n de endpoints sensibles (/admin, /metrics, etc.)

¬øPor qu√© sirve?

No ten√©s acceso por c√≥digo a estas reglas.

Pod√©s trackear cambios con Git.

Te ayuda a repetir o revisar configuraciones si migr√°s de cuenta, dominio, etc.


=== File: infra/dev_docs/notes.md ===
infra/ representa infraestructura pasiva/configurable: certificados, reglas de firewall, YAMLs de Terraform, etc. pensada para scripts, claves, secretos, configuraciones externas.

Esto se diferencia de net/ que representa servicios activos, como api_gateway, auth_proxy, webhook_ingress, etc.


=== File: infra/firewall_rules.sh ===


=== File: net/api_gateway/dev_docs/notes.md ===
net/ que representa servicios activos, como api_gateway, auth_proxy, webhook_ingress, etc. a diferencia de infra/ representa infraestructura pasiva/configurable: certificados, reglas de firewall, YAMLs de Terraform, etc., pensada para scripts, claves, secretos, configuraciones externas.


=== File: servers/admin_panel/app/__init__.py ===


=== File: servers/admin_panel/app/kafka_producer.py ===
import json
from confluent_kafka import Producer
from datetime import datetime, timezone
import os

# VARIABLES DE KAFKA
# Se leen de entorno si est√°n definidas, sino usamos valores por defecto:
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092")
KAFKA_TOPIC = os.getenv("KAFKA_TOPIC", "configUpdate")

producer: Producer | None = None


def init_kafka_producer():
    """
    Inicializa un Producer global conectado al broker de Kafka.
    """
    global producer
    if producer is None:
        producer = Producer({"bootstrap.servers": KAFKA_BOOTSTRAP_SERVERS})


# En app/kafka_producer.py, reemplaza test_kafka_connection() por:
def test_kafka_connection():
    if producer is None:
        raise RuntimeError("Producer no inicializado")
    try:
        metadata = producer.list_topics(timeout=5.0)
    except Exception as e:
        print(f"[WARNING] No se pudo obtener metadata de Kafka: {e}")
        return
    # Si el t√≥pico no existe, no intentamos crearlo aqu√≠
    if KAFKA_TOPIC not in metadata.topics:
        print(
            f"[WARNING] T√≥pico '{KAFKA_TOPIC}' no encontrado, y no se crea autom√°ticamente."
        )


def publish_config_update(service: str, version: int, payload: dict):
    """
    Publica un evento de configuraci√≥n para 'service' con una versi√≥n y payload.
    :param service: nombre del microservicio (ej: "file_service")
    :param version: n√∫mero incremental de versi√≥n
    :param payload: diccionario { "VAR1": "valor1", ... }
    """
    if producer is None:
        raise RuntimeError("Producer de Kafka no inicializado")

    message = {
        "service": service,
        "version": version,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "payload": payload,
    }

    producer.produce(
        topic=KAFKA_TOPIC,
        key=service.encode("utf-8"),
        value=json.dumps(message).encode("utf-8"),
    )
    # Podemos hacer poll para procesar callbacks, o flush si queremos sincr√≥nico
    producer.poll(0)


=== File: servers/admin_panel/app/main.py ===
# app/main.py

from fastapi import FastAPI, HTTPException
from app.routers.config import router as config_router
from app.kafka_producer import (
    init_kafka_producer,
    test_kafka_connection,
    publish_config_update,
    KAFKA_BOOTSTRAP_SERVERS,
    KAFKA_TOPIC,
)
from config.env_loader import load_env_config

app = FastAPI(title="admin_panel")


@app.on_event("startup")
def startup_event():
    try:
        init_kafka_producer()
        env_config = load_env_config()
        publish_config_update(service="admin_panel", version=1, payload=env_config)
        print("[INFO] Configuraci√≥n publicada en Kafka.")
    except Exception as e:
        print(f"[WARNING] Error al publicar configuraci√≥n inicial: {e}")


app.include_router(config_router, prefix="/config", tags=["config"])


@app.get("/healthz")
async def healthz():
    return {
        "status": "ok",
        "kafka_bootstrap": KAFKA_BOOTSTRAP_SERVERS,
        "kafka_topic": KAFKA_TOPIC,
    }


=== File: servers/admin_panel/app/models.py ===
from pydantic import BaseModel
from typing import Dict
from datetime import datetime


class ConfigUpdate(BaseModel):
    service: str
    version: int
    payload: Dict[str, str]
    timestamp: datetime

    class Config:
        json_schema_extra = {
            "example": {
                "service": "file_service",
                "version": 1,
                "timestamp": "2025-05-30T23:45:00Z",
                "payload": {
                    "FILE_BASE_PATH": "/data/files",
                    "KAFKA_BOOTSTRAP": "kafka:9092",
                    "KAFKA_TOPIC": "file_stored",
                    "MAX_UPLOAD_MB": "100",
                },
            }
        }


=== File: servers/admin_panel/app/routers/config.py ===
from fastapi import APIRouter, HTTPException, status
from app.schemas import ConfigPayload
from app.kafka_producer import publish_config_update

router = APIRouter()


@router.post("/{service_name}", status_code=status.HTTP_200_OK)
async def update_config(service_name: str, body: ConfigPayload):
    """
    Endpoint: POST /config/{service_name}
    Recibe JSON {"version": int, "payload": { key: value, ... }}.
    Publica un mensaje en Kafka para que el microservicio 'service_name'
    reciba en tiempo real la configuraci√≥n.
    """
    try:
        publish_config_update(
            service=service_name, version=body.version, payload=body.payload
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error al publicar actualizaci√≥n de configuraci√≥n: {e}",
        )

    return {"detail": f"Configuraci√≥n enviada para servicio '{service_name}'"}


=== File: servers/admin_panel/app/schemas.py ===
from pydantic import BaseModel
from typing import Dict


class ConfigPayload(BaseModel):
    version: int
    payload: Dict[str, str]

    class Config:
        json_schema_extra = {
            "example": {
                "version": 2,
                "payload": {
                    "FILE_BASE_PATH": "/data/files",
                    "KAFKA_BOOTSTRAP": "kafka:9092",
                    "KAFKA_TOPIC": "file_stored",
                    "MAX_UPLOAD_MB": "100",
                },
            }
        }


=== File: servers/admin_panel/dev_docs/README.md ===
# admin_panel

Microservicio para gestionar configuraci√≥n centralizada v√≠a Kafka.

## Requisitos previos

- Python 3.10+
- Docker (solo para Kafka y Zookeeper)

## 1. Levantar Kafka y Zookeeper en Docker

```bash
# Desde la ra√≠z del proyecto (o donde est√© tu docker-compose.yml)
docker network create walsh_net         # si no existe
docker compose -f services/kafka/docker-compose.yml up -d
```


=== File: services/elasticsearch/docker-compose.elasticsearch.yml ===
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.2
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - esdata:/usr/share/elasticsearch/data
    networks:
      - internal

  search_service:
    build: ./services/search_service
    depends_on:
      - elasticsearch
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
    networks:
      - internal

volumes:
  esdata:

networks:
  internal:


=== File: services/file_service/app/events.py ===
# app/events.py

from confluent_kafka import Producer
from datetime import datetime, timezone
from app.models import FileMeta, FileStoredEvent
from config.variables.file_storage import KAFKA_BOOTSTRAP, KAFKA_TOPIC
import json
import logging

logger = logging.getLogger(__name__)


def get_producer() -> Producer:
    return Producer({"bootstrap.servers": KAFKA_BOOTSTRAP})


def publish_file_stored_event(key: str, meta: FileMeta):
    producer = get_producer()
    event = FileStoredEvent(key=key, meta=meta, timestamp=datetime.now(timezone.utc))
    json_payload = event.model_dump_json()
    try:
        producer.produce(
            topic=KAFKA_TOPIC, key=key.encode(), value=json_payload.encode()
        )
        producer.poll(0)
    except Exception as e:
        logger.exception(f"Error al publicar evento Kafka: {e}")


def test_kafka_connection():
    producer = get_producer()
    metadata = producer.list_topics(timeout=5)
    if KAFKA_TOPIC not in metadata.topics:
        raise RuntimeError(f"T√≥pico {KAFKA_TOPIC} no existe")


=== File: services/file_service/app/main.py ===
# app/main.py

from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from prometheus_client import make_asgi_app
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.responses import JSONResponse
from app.routes.files import router as files_router
from app.events import test_kafka_connection
import os
from config.variables.security import ALLOWED_EXTENSIONS


settings = get_settings()
app = FastAPI(title="file_service")

# Middleware: CORS (si se necesita)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Middleware: limitador de tama√±o de archivo
class UploadLimitMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        content_length = request.headers.get("content-length")
        if (
            content_length
            and int(content_length) > settings.MAX_UPLOAD_MB * 1024 * 1024
        ):
            return JSONResponse(
                status_code=413,
                content={
                    "detail": f"Archivo supera el l√≠mite de {settings.MAX_UPLOAD_MB} MB"
                },
            )
        return await call_next(request)


app.add_middleware(UploadLimitMiddleware)

# Rutas API
app.include_router(files_router, prefix="", tags=["files"])


# Rutas de salud
@app.get("/healthz")
async def healthz():
    # Verifica si se puede escribir en disco
    try:
        test_file = os.path.join(settings.FILE_BASE_PATH, ".healthcheck")
        with open(test_file, "w") as f:
            f.write("ok")
        os.remove(test_file)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Storage error: {e}")

    # Verifica conexi√≥n Kafka
    try:
        test_kafka_connection()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Kafka error: {e}")

    return {"status": "ok"}


# Rutas de m√©tricas
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)


=== File: services/file_service/app/models.py ===
# app/models.py

from pydantic import BaseModel
from typing import List
from datetime import datetime


class FileMeta(BaseModel):
    user_id: str
    tags: List[str]
    sha256: str
    created_at: datetime


class FileStoredEvent(BaseModel):
    key: str
    meta: FileMeta
    timestamp: datetime


=== File: services/file_service/app/routes/files.py ===
# app/routes/files.py

from fastapi import APIRouter, UploadFile, File, HTTPException, Request
from fastapi.responses import StreamingResponse, Response
from hashlib import sha256
from datetime import datetime, timezone
from mimetypes import guess_type
import aiofiles, os
from app.models import FileMeta
from app.storage.local import LocalFSBackend
from app.events import publish_file_stored_event
from secrets import token_hex
from datetime import datetime
from secrets import token_hex
from config.variables.file_storage import settings

producer = Producer({"bootstrap.servers": settings.KAFKA_BOOTSTRAP})


router = APIRouter()
storage = LocalFSBackend(FILE_BASE_PATH)


def generar_clave_archivo(
    origen: str, extension: str, base_path: str, max_reintentos: int = 10
) -> str:
    fecha = datetime.now(timezone.utc).strftime("%Y%m%d")
    hora_min = datetime.now(timezone.utc).strftime("%H%M")
    for _ in range(max_reintentos):
        short_id = token_hex(3)  # 6 caracteres
        nombre_archivo = f"{hora_min}-{short_id}{extension}"
        clave = f"{origen}/{fecha}/{nombre_archivo}"
        ruta = os.path.join(base_path, clave)
        if not os.path.exists(ruta):
            return clave
    raise RuntimeError(
        f"No se pudo generar un nombre √∫nico despu√©s de {max_reintentos} intentos"
    )


@router.post("/files")
async def upload_file(request: Request, file: UploadFile = File(...)):
    user_id = request.headers.get("x-user-id", "anonymous")
    origen = request.headers.get("x-source", "unknown")  # microservicio llamante
    content = await file.read()

    extension = os.path.splitext(file.filename or "")[1].lower()

    try:
        key = generar_clave_archivo(origen, extension, FILE_BASE_PATH)
        storage.save_file(key, content)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error al guardar archivo: {e}")

    hash_hex = sha256(content).hexdigest()
    meta = FileMeta(
        user_id=user_id, tags=[], sha256=hash_hex, created_at=datetime.utcnow()
    )
    publish_file_stored_event(key, meta)

    url = f"{FILE_PUBLIC_URL}/{key}"
    return {"key": key, "url": url}


@router.get("/files/{key}")
async def get_file(key: str):
    try:
        path = os.path.join(FILE_BASE_PATH, key)
        if not os.path.exists(path):
            raise HTTPException(status_code=404, detail="Archivo no encontrado")

        mime, _ = guess_type(path)
        mime = mime or "application/octet-stream"

        async def file_iterator():
            async with aiofiles.open(path, mode="rb") as f:
                while chunk := await f.read(8192):
                    yield chunk

        return StreamingResponse(file_iterator(), media_type=mime)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error al leer archivo: {e}")


@router.head("/files/{key}")
async def head_file(key: str):
    path = os.path.join(FILE_BASE_PATH, key)
    if not os.path.exists(path):
        raise HTTPException(status_code=404, detail="Archivo no encontrado")

    size = os.path.getsize(path)
    mime, _ = guess_type(path)
    mime = mime or "application/octet-stream"

    headers = {
        "Content-Length": str(size),
        "Content-Type": mime,
    }
    return Response(headers=headers)


@router.delete("/files/{key}")
async def delete_file(key: str):
    path = os.path.join(FILE_BASE_PATH, key)
    if not os.path.exists(path):
        raise HTTPException(status_code=404, detail="Archivo no encontrado")
    try:
        os.remove(path)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"No se pudo eliminar: {e}")
    return {"detail": "Archivo eliminado"}


=== File: services/file_service/app/storage/backend.py ===
# app/storage/backend.py

from abc import ABC, abstractmethod


class StorageBackend(ABC):
    """Interfaz abstracta para backends de almacenamiento."""

    @abstractmethod
    def save_file(self, key: str, data: bytes) -> None:
        """Guarda un archivo binario bajo una clave √∫nica."""
        pass

    @abstractmethod
    def get_file(self, key: str) -> bytes:
        """Recupera un archivo binario por clave."""
        pass

    @abstractmethod
    def delete_file(self, key: str) -> None:
        """Elimina un archivo almacenado (l√≥gico o f√≠sico)."""
        pass

    @abstractmethod
    def file_exists(self, key: str) -> bool:
        """Verifica si un archivo existe."""
        pass


=== File: services/file_service/app/storage/local.py ===
# app/storage/local.py

import os
from pathlib import Path
from app.storage.backend import StorageBackend


class LocalFSBackend(StorageBackend):
    """Implementaci√≥n del backend que guarda archivos en el sistema de archivos local."""

    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)

    def _resolve_path(self, key: str) -> Path:
        return self.base_path / key

    def save_file(self, key: str, data: bytes) -> None:
        path = self._resolve_path(key)
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_bytes(data)

    def get_file(self, key: str) -> bytes:
        path = self._resolve_path(key)
        if not path.exists():
            raise FileNotFoundError(f"Archivo {key} no encontrado")
        return path.read_bytes()

    def delete_file(self, key: str) -> None:
        path = self._resolve_path(key)
        if path.exists():
            path.unlink()

    def file_exists(self, key: str) -> bool:
        return self._resolve_path(key).exists()


=== File: services/file_service/app/tests/test_files_api.py ===
# app/tests/test_files_api.py

import pytest
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)


def test_upload_and_download_file(tmp_path, monkeypatch):
    monkeypatch.setenv("FILE_BASE_PATH", str(tmp_path))

    content = b"contenido de prueba"
    response = client.post(
        "/files",
        files={"file": ("archivo.txt", content)},
        headers={"x-user-id": "test_user"},
    )
    assert response.status_code == 200
    data = response.json()
    assert "key" in data
    key = data["key"]

    # Obtener archivo
    get_response = client.get(f"/files/{key}")
    assert get_response.status_code == 200
    assert get_response.content == content


=== File: services/file_service/app/tests/test_storage.py ===


=== File: services/file_service/dev_docs/otros_agregados.md ===
Mejorar el esquema del evento Kafka file_stored: podr√≠a definir expl√≠citamente el modelo FileStoredEvent tambi√©n para serializar a JSON (hoy lo arma como un diccionario).
Agregar limitador de tama√±o de archivo (MAX_UPLOAD_MB) en el middleware.


Mi recomendaci√≥n pragm√°tica:

### 1. Soporte S3/NFS

* **C√≥digo listo, pero backend local por defecto.**

  * Define desde el inicio una interfaz `StorageBackend` con dos m√©todos (`save_file`, `get_file`).
  * Implement√° la versi√≥n **LocalFSBackend** ahora mismo.
  * Dej√° stubs vac√≠os o un m√≥dulo `s3_backend.py` / `nfs_backend.py` con TODO-comments; as√≠ el patr√≥n ya existe y la migraci√≥n futura s√≥lo ser√° agregar la implementaci√≥n y ajustar una variable de entorno (`STORAGE_BACKEND=s3`).
  * Ventaja: manten√©s el servicio liviano y sin dependencias pesadas (boto3, fsspec) en la beta, pero la arquitectura ya contempla el intercambio de backend sin refactor.

### 2. Cliente de Kafka

* **Producci√≥n** ‚Üí `confluent-kafka` (enlaza librdkafka en C).

  * \~10√ó m√°s throughput que `kafka-python`, menor latencia y mejor compresi√≥n.
  * Mantenimiento activo por Confluent, preparaci√≥n para features nuevos (idempotent producer, transactions).
* **Desarrollo local** ‚Üí pod√©s empezar tambi√©n con `confluent-kafka`; hoy est√° en PyPI y la instalaci√≥n v√≠a wheels pre-compilados suele ser trivial en Ubuntu.

  * Si alguna vez el build se complica (p.ej. en Windows sin librdkafka), `kafka-python` sirve como fallback sin cambios de API mayores.

**En resumen:**

* Arranc√° con backend local implementado, pero con interfaz pluggable y flag para futuro S3/NFS.
* Usa `confluent-kafka` como cliente principal; te ahorra dolores de rendimiento cuando escales y no agrega complejidad significativa durante el desarrollo.



=== File: services/file_service/docker-compose.yml ===
version: '3.8'

services:
  file_service:
    build: .
    ports:
      - "8004:8004"
    environment:
      - FILE_BASE_PATH=/data/files
      - FILE_PUBLIC_URL=http://localhost:8004/files
      - KAFKA_BOOTSTRAP=kafka:9092
      - KAFKA_TOPIC=file_stored
      - MAX_UPLOAD_MB=50
    volumes:
      - ./data/files:/data/files
    depends_on:
      - kafka
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000


# DESCOMENTAR EN CASO DE USAR VOLUMENES DENTRO DEL CONTENEDOR
# volumes:
#   - file_data:/data/files

=== File: services/kafka/README.md ===
**EJECUTAR POR DEFAULT AL INICIO**
```
docker ps #CHEQUEAR CONTENEDORES UP
cd /home/admin1/Desktop/proyectos_de_desarrollo/walsh/backend  #MOVER AL DIRECTORIO PRINCIPAL DEL PROYECTO
docker compose -f ./services/kafka/docker-compose.yml down --remove-orphans #MATAR CONTENEDORES PREVIOS
docker compose -f ./services/kafka/docker-compose.yml up -d #LEVANTAR KAFKA, KAFKA-UI Y ZOOKEEPER
./services/kafka/create_topics.sh #CREAR TOPICS
```

**CONTENEDORES UP DE DOCKER**
docker ps

**MATAR CONTENEDORES**
docker compose -f ./services/kafka/docker-compose.yml down --remove-orphans

**LEVANTAR KAFKA, KAFKA UI Y ZOOKEEPER**
docker compose -f ./services/kafka/docker-compose.yml up -d

**VER KAFKA UI**
http://localhost:8080


**VER LOGS DEL CONTENEDOR KAFKA: SI HAY ERROR, DEBE DECIRLO ACA**
docker compose -f ./services/kafka/docker-compose.yml logs kafka


**LEVANTAR RED DE DOCKER**
docker network create walsh_net


**LAUNCH MICROSERVICE**
*En la carpeta del directorio del microservicio:*

docker compose build
docker compose up -d

=== File: services/kafka/create_topics.sh ===
#!/usr/bin/env bash
# services/kafka/create_topics.sh
set -e

COMPOSE_FILE="services/kafka/docker-compose.yml"
TOPICS=(
  configUpdate
  logs
  fileStored
  rawDocuments
  documentEnriched
  userCreated
  keyRegistered
  chatMessage
)

docker compose -f "$COMPOSE_FILE" exec broker bash -c '
  for t in '"${TOPICS[*]}"'; do
    kafka-topics --bootstrap-server broker:29092 \
      --create --if-not-exists --topic "$t" \
      --partitions 1 --replication-factor 1
  done
'
echo "‚úÖ Topics creados (o ya exist√≠an)"


=== File: services/kafka/docker-compose.yml ===
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-kafka:7.6.0          # usa la comunidad, sin licencia
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"   # interno ‚Üí red docker (broker:29092)
      - "9092:9092"     # externo ‚Üí host      (localhost:9092)
    environment:
      # *** ¬°IMPORTANTE!  todo en una sola l√≠nea y entre comillas ***
      KAFKA_LISTENERS: "INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092"
      KAFKA_ADVERTISED_LISTENERS: "INTERNAL://broker:29092,EXTERNAL://localhost:9092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - broker
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker:29092
      KAFKA_CLUSTERS_0_ZOOKEEPERCONNECT: zookeeper:2181
